[GENERAL]
ckpt_path = './ckpt/'     # Root folder for checkpoints (model weights)
outp_path = './outputs/'  # Root folder for general outputs (img predictions, generated train sparse masks, etc)
model = 'unet'            # Architecture used, at moment, only miniUnet is implemented 
dataset = 'openist'          # Few-shot dataset
task = 'both_lungs'       # Few-shot target class
fold = '0'                # Fold used in training

# Will perform the training of the few-shot task with all these configurations
[FEW-SHOT]
list_shots = [1, 5, 10, 20]                                 # Number of shots in the task (i.e, total annotated sparse samples)
list_sparsity_points = [1, 5, 10, 20]                       # Number of labeled pixels in point annotation
list_sparsity_contours = [0.05, 0.10, 0.25, 0.50, 1.00]     # Density of the contours (1, is the complete contours)
list_sparsity_grid = [8, 12, 16, 20]                        # Spacing between selected pixels in grid annotation
list_sparsity_regions = [0.05, 0.10, 0.25, 0.50, 1.00]      # Percentege of regions labeled (1, all \pure\ regions are labeled)
list_sparsity_skels = [0.05, 0.10, 0.25, 0.50, 1.00]        # Density of the skeletons (1, is the complete skeletons)

# List of dictionaries with all possible tasks for meta-training. The few-shot task is ignored in training
# domain: is the name of dataset; task: is the positive/target class
# The class ListDataset use these infos to load the respectives dataset accordingly, modify this class to your use case
# For a simple loader, this version is implemented considering the following folder scheme:

# /datasets_root_folder
# |--- dataset_1
#      |--- images
#      |--- ground_truths
# |--- dataset_2
#      |--- images
#      |--- ground_truths
# .
# .
# .
# |--- dataset_n
#      |--- images
#      |--- ground_truths
[TASKS]
task_dicts = [
                {'domain': 'jsrt', 'task': 'both_lungs'},
                {'domain': 'jsrt', 'task': 'both_clavicles'},
                {'domain': 'jsrt', 'task': 'heart'},
                
                {'domain': 'openist', 'task': 'both_lungs'},
                
                {'domain': 'montgomery', 'task': 'both_lungs'},
                {'domain': 'shenzhen', 'task': 'both_lungs'},
                {'domain': 'nih_labeled', 'task': 'both_lungs'},
                
                {'domain': 'lidc_idri_drr', 'task': 'ribs'},
                
                {'domain': 'ufba', 'task': 'teeth'},
                {'domain': 'panoramic', 'task': 'mandible'},
                
                {'domain': 'inbreast', 'task': 'pectoral'},
                {'domain': 'inbreast', 'task': 'breast'},
                
                {'domain': 'mias', 'task': 'pectoral'},
                {'domain': 'mias', 'task': 'breast'},
            ]

[DATA]
num_channels = 1 # Number of channels the images have (medical are grayscale, so use 1)
num_class = 2    # Number of classes. We only solve binary problems, so 2
imgtype = 'med'  # Flag of imagetype, can be used for loading different types of images for tasks


[TRAINING]        
epoch_num = 200                 # Number of epochs.
lr = 1e-3                       # Learning rate.
weight_decay = 5e-5             # L2 penalty.
momentum = 0.9                  # Momentum.
num_workers = 0                 # Number of workers on data loader.
snapshot = ''                   # Starting epoch to resume training. Previously saved weights are loaded.
batch_size = 5                  # Mini-batch size.
w_size = 256                    # Width size for image resizing.
h_size = 256                    # Height size for image resizing.
test_freq = 200                 # Run tuning each test_freq epochs.
first_order = False             # First order approximation of MAML.
step_size = 0.3                 # MAML inner loop step size.
n_metatasks_iter = 4            # Number of randomly sampled tasks in meta-learning.
tuning_epochs = 40              # Number of epochs on the tuning phase.
tuning_freq = 4                 # Test each tuning_freq epochs on the tuning phase.
lr_scheduler_step_size = 150
lr_scheduler_gamma = 0.2

[GUIDED]
mapping = 'learned'             # Mapping function of the guided nets. 'learned' (Learned map), 'simple' (Direct map (bilinear interpolation if necessary))
loss_fn = 'SCE'                 # Loss function. 'SCE', 'Dice', or 'SCE+Dice'